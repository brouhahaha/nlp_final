{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsC2ab7Fn0W3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "868cbdbb-5aed-47c4-f455-8ae221200273"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dir_path = '/content/drive/My Drive/автобрея/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5pX6yNjufkS",
        "colab_type": "text"
      },
      "source": [
        "Необходимые библиотеки и т.д."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Ikwsgtn4_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "36339b1f-8f1f-4687-d02e-12b38f2d7922"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 21.9MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Installing collected packages: pymorphy2-dicts, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW1dd5iLn5HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDpARs3Qn5J3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "27ab0298-af8e-4263-df03-fa1722d8f7bb"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from pymystem3 import Mystem\n",
        "import re\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "token = RegexpTokenizer('\\w+')\n",
        "stops = set(stopwords.words('russian'))\n",
        "\n",
        "def normalize_pm(text):\n",
        "    words = [morph.parse(word)[0].normal_form for word in tokenize(text) if word]\n",
        "    return words\n",
        "\n",
        "def tokenize(text):\n",
        "    return token.tokenize(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwKt7w8An5Mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "1a79611c-d82f-4b86-97f2-bd08f91fff18"
      },
      "source": [
        "!pip install git+https://github.com/lopuhin/python-adagram.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-jhsnjka6\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-jhsnjka6\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.29.14)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp36-cp36m-linux_x86_64.whl size=470373 sha256=89e0512a3165d855df8cfa23125f109d935461dd3f696c0ede8ccb33a2ad100c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oy3jc98s/wheels/11/0f/46/f5df96670df8f7973b4c2311ffc9b02e435a7bd3207f992c4d\n",
            "Successfully built adagram\n",
            "Installing collected packages: adagram\n",
            "Successfully installed adagram-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB3bJGX5oJQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3bc43f6-438b-4372-95ee-fa01eb7d70af"
      },
      "source": [
        "!wget https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib -d adagram.joblib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG output created by Wget 1.19.4 on linux-gnu.\n",
            "\n",
            "Reading HSTS entries from /root/.wget-hsts\n",
            "URI encoding = ‘UTF-8’\n",
            "Converted file name 'all.a010.p10.d300.w5.m100.nonorm.slim.joblib' (UTF-8) -> 'all.a010.p10.d300.w5.m100.nonorm.slim.joblib' (UTF-8)\n",
            "--2019-12-24 13:54:24--  https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.95.21\n",
            "Caching s3.amazonaws.com => 52.216.95.21\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.95.21|:443... connected.\n",
            "Created socket 5.\n",
            "Releasing 0x000055a8c1ff0a60 (new refcount 1).\n",
            "Initiating SSL handshake.\n",
            "Handshake successful; connected socket 5 to SSL handle 0x000055a8c2014000\n",
            "certificate:\n",
            "  subject: CN=s3.amazonaws.com,O=Amazon.com\\\\, Inc.,L=Seattle,ST=Washington,C=US\n",
            "  issuer:  CN=DigiCert Baltimore CA-2 G2,OU=www.digicert.com,O=DigiCert Inc,C=US\n",
            "X509 certificate successfully verified and matches host s3.amazonaws.com\n",
            "\n",
            "---request begin---\n",
            "GET /kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib HTTP/1.1\r\n",
            "User-Agent: Wget/1.19.4 (linux-gnu)\r\n",
            "Accept: */*\r\n",
            "Accept-Encoding: identity\r\n",
            "Host: s3.amazonaws.com\r\n",
            "Connection: Keep-Alive\r\n",
            "\r\n",
            "---request end---\n",
            "HTTP request sent, awaiting response... \n",
            "---response begin---\n",
            "HTTP/1.1 200 OK\r\n",
            "x-amz-id-2: AkX6pAsQ+47/txrxrZkkt6p1hrnS+ZA2UU32mvXu3kUnBj/soLSmr3isJvbaNzACnGxLSxz1mzc=\r\n",
            "x-amz-request-id: 8503A29CF27F0857\r\n",
            "Date: Tue, 24 Dec 2019 13:54:25 GMT\r\n",
            "Last-Modified: Mon, 30 Oct 2017 18:17:18 GMT\r\n",
            "ETag: \"f5f49c3c6ebb8e0578161cad1a6ab2d1-88\"\r\n",
            "Accept-Ranges: bytes\r\n",
            "Content-Type: application/x-www-form-urlencoded; charset=utf-8\r\n",
            "Content-Length: 1462416741\r\n",
            "Server: AmazonS3\r\n",
            "\r\n",
            "---response end---\n",
            "200 OK\n",
            "Registered socket 5 for persistent reuse.\n",
            "URI content encoding = ‘utf-8’\n",
            "Length: 1462416741 (1.4G) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘all.a010.p10.d300.w5.m100.nonorm.slim.joblib’\n",
            "\n",
            "all.a010.p10.d300.w 100%[===================>]   1.36G  35.0MB/s    in 40s     \n",
            "\n",
            "2019-12-24 13:55:04 (34.5 MB/s) - ‘all.a010.p10.d300.w5.m100.nonorm.slim.joblib’ saved [1462416741/1462416741]\n",
            "\n",
            "URI encoding = ‘UTF-8’\n",
            "Converted file name 'index.html' (UTF-8) -> 'index.html' (UTF-8)\n",
            "--2019-12-24 13:55:04--  http://adagram.joblib/\n",
            "Resolving adagram.joblib (adagram.joblib)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘adagram.joblib’\n",
            "FINISHED --2019-12-24 13:55:05--\n",
            "Total wall clock time: 41s\n",
            "Downloaded: 1 files, 1.4G in 40s (34.5 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU_SFMCwoJUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import adagram\n",
        "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnPy0E_yoJS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGOIwCFBs6C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim, logging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPHIIiD9s6qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I_v6b3ws730",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "bbe419e5-3b4c-41c3-81e1-fc4dcc634277"
      },
      "source": [
        "amodel = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/автобрея/model.bin\", binary=True)\n",
        "amodel.init_sims(replace=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-24 13:55:08,342 : INFO : loading projection weights from /content/drive/My Drive/автобрея/model.bin\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2019-12-24 13:55:14,471 : INFO : loaded (248978, 300) matrix from /content/drive/My Drive/автобрея/model.bin\n",
            "2019-12-24 13:55:14,472 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx3aN8BhoYyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## достаем наши сиды и сохраняем в переменные (все эти данные сохранены в файлах pickle)\n",
        "with open(\"/content/drive/My Drive/автобрея/Food_final_seeds.pickle\", \"rb\") as c: \n",
        "  food_seeds = pickle.load(c)\n",
        "\n",
        "with open(\"/content/drive/My Drive/автобрея/Service_final_seeds.pickle\", \"rb\") as c:\n",
        "  service_seeds = pickle.load(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQCeuiPeqgaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Достаем вектора топиков\n",
        "with open(\"/content/drive/My Drive/автобрея/Service_topic_vector.pickle\", \"rb\") as c:\n",
        "  service_topic_vector = pickle.load(c)\n",
        "\n",
        "with open(\"/content/drive/My Drive/автобрея/Food_topic_vector.pickle\", \"rb\") as c:\n",
        "  food_topic_vector = pickle.load(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyDmXKxaq1Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## достаем векторы негативных и позитивных тональных слов\n",
        "with open(\"/content/drive/My Drive/автобрея/Positive_vector.pickle\", \"rb\") as c:\n",
        "  pos_vector = pickle.load(c)\n",
        "\n",
        "with open(\"/content/drive/My Drive/автобрея/Negative_vector.pickle\", \"rb\") as c:\n",
        "  neg_vector = pickle.load(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWNsQxcpxCZ",
        "colab_type": "text"
      },
      "source": [
        "Классификатор:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJDvsDsVsyEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#словарь тэгов для конвертации из формата pymorphy в w2v\n",
        "tags = { 'INFN':'VERB', 'NOUN':'NOUN', 'ADJF':'ADJ', 'ADJS':'ADJ','PREP':'ADP', 'ADVB':'ADV', 'CONJ':'SCONJ', 'PRCL':'PART', 'NUMR':'NUM', 'NPRO':'PRON', 'INTJ':'INTJ', 'PRED':'ADV', 'VERB':'VERB', 'GRND':'GRND', None:'None', 'PRTF':'PRTF', 'PRTS':'PRTS'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9UHlWiPsy2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_word(word):                 #в модели вордувек слова хранятся так: дом_NOUN - эта функция преобразует слово в такой формат\n",
        "    aword = re.sub('ё', \"е\", word)\n",
        "    p = morph.parse(word)[0]\n",
        "    tag = tags[p.tag.POS]\n",
        "    new_word = aword+'_'+tag\n",
        "    return new_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS7IiVs5qfsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_topics(file, service_vector, food_vector):\n",
        "  food_class_r = {}                             \n",
        "  service_class_r = {}\n",
        "\n",
        "  with open(file, 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n",
        "    sents = re.split('(?<=[.!?]) (?=[A-ЯЁA-Z])', text)\n",
        "\n",
        "    for sent in sents:\n",
        "      for word in sent.split():\n",
        "        norm = ' '.join(normalize_pm(word))\n",
        "        if norm not in stops and convert_word(norm) in amodel.wv.vocab:\n",
        "\n",
        "          word_vector = amodel.wv[convert_word(norm)]\n",
        "          sim_service = spatial.distance.cosine(word_vector, service_vector)\n",
        "          sim_food = spatial.distance.cosine(word_vector, food_vector)\n",
        "\n",
        "          if sim_service<sim_food:\n",
        "            service_class_r[(norm, sim_service)] = sents.index(sent)\n",
        "          elif sim_service>sim_food:\n",
        "            food_class_r[(norm, sim_food)] = sents.index(sent)\n",
        "  return sents, food_class_r, service_class_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QECag-qyT7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier1(file, result = set()): ## можно вместо пустого резалта давать ему резалт из другого классифаера и они будут объединяться\n",
        "  with open(file, 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n",
        "    sents = re.split('(?<=[.!?]) (?=[A-ЯЁA-Z])', text)\n",
        "    \n",
        "  for sent in sents:\n",
        "    try:\n",
        "      sent_words = [normalize_pm(w.strip())[0] for w in sent.split()]\n",
        "    except:\n",
        "      print('Cannot handle sentence ' + str(sents.index(sent) + 1))\n",
        "    ## ищем все тональное о еде:\n",
        "    for line in food_seeds:\n",
        "      t_word = normalize_pm(line[0].strip())[0]\n",
        "      tone = bool(int(line[1])) # так удобно их преобразовывать, если встр. \"не\"\n",
        "      if t_word in sent_words:\n",
        "        id_sent = sents.index(sent) + 1 # прибавляем 1, т.к. в юдпайпе счет с одного\n",
        "        i = sent_words.index(t_word)\n",
        "        try:\n",
        "          if sent_words[i-1] == 'не':\n",
        "            id_tone = str(i) + ',' + str(i+1) # плюс один по тем же причинам\n",
        "            tone = int(not tone)\n",
        "          elif sent_words[i-2] == 'не':\n",
        "            id_tone = str(i-1) + ',' + str(i) + ',' + str(i+1) # тоже\n",
        "            tone = int(not tone)\n",
        "          else:\n",
        "            id_tone = str(i+1) # тож\n",
        "            tone = int(tone)\n",
        "        except IndexError:\n",
        "          pass\n",
        "        current = str(id_sent) + '\\t' + id_tone + '\\t' + 'Food\\t' + str(tone)\n",
        "        result.add(current)\n",
        "    ## ищем все тональное о сервисе:\n",
        "    for line in service_seeds:\n",
        "      t_word = normalize_pm(line[0].strip())[0]\n",
        "      tone = bool(int(line[1])) # так удобно их преобразовывать, если встр. \"не\"\n",
        "      if t_word in sent_words:\n",
        "        id_sent = sents.index(sent) + 1 # прибавляем 1, т.к. в юдпайпе счет с одного\n",
        "        i = sent_words.index(t_word)\n",
        "        try:\n",
        "          if sent_words[i-1] == 'не':\n",
        "            id_tone = str(i) + ',' + str(i+1) # плюс один по тем же причинам\n",
        "            tone = int(not tone)\n",
        "          elif sent_words[i-2] == 'не':\n",
        "            id_tone = str(i-1) + ',' + str(i) + ',' + str(i+1) # тоже\n",
        "            tone = int(not tone)\n",
        "          else:\n",
        "            id_tone = str(i+1) # тож\n",
        "            tone = int(tone)\n",
        "        except IndexError:\n",
        "          pass\n",
        "        current = str(id_sent) + '\\t' + id_tone + '\\t' + 'Service\\t' + str(tone)\n",
        "        result.add(current)\n",
        "  return(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7fXF_g6S6r6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vect_sim(word, vector1, vector2):\n",
        "  sim1 = spatial.distance.cosine(amodel.wv[convert_word(word)], vector1)\n",
        "  sim2 = spatial.distance.cosine(amodel.wv[convert_word(word)], vector2)\n",
        "  return sim1, sim2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPkrsL-AuSDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier2(file, result = set()): \n",
        "  ## можно вместо пустого резалта давать ему резалт из другого классифаера и они будут объединяться\n",
        "  \"\"\"\n",
        "  Например, можно будет сделать так:\n",
        "  Сверху у нас уже есть simple_result первого классификатора на тексте 6668\n",
        "  Можно запустить классификатор два на этом результате\n",
        "  result = classifier2(file, result = simple_result)\n",
        "  Тогда классификатор2 будет добавлять свои штучки в то же множество, и данные не будут повторяться\n",
        "\n",
        "  И вообще потом эти функции можно запускать на куче текстов друг за другом\n",
        "  \"\"\"\n",
        "  sents, food_class_r, service_class_r = get_topics(file, service_topic_vector, food_topic_vector)\n",
        "\n",
        "  for obj in food_class_r:\n",
        "    id_sent = food_class_r[obj]\n",
        "    sent = re.sub ('- ', '', sents[id_sent])\n",
        "    sent_words = [normalize_pm(w.strip())[0] for w in sent.split()]\n",
        "    asp_label = 'Food'\n",
        "    word = obj[0]\n",
        "    p = morph.parse(word)[0]\n",
        "    pos = p.tag.POS\n",
        "    id_word = sent_words.index(word)\n",
        "    #print(sent)\n",
        "\n",
        "    if pos == 'ADVB':\n",
        "      sim_pos, sim_neg = vect_sim(word, pos_vector, neg_vector)\n",
        "      if sim_pos < sim_neg:\n",
        "        label = 1\n",
        "      else: \n",
        "        label = 0\n",
        "      current = str(id_sent + 1) + '\\t' + str(id_word + 1) + '\\t' + str(asp_label) + '\\t' + str(label)\n",
        "      result.add(current)\n",
        "      #print(current, word)\n",
        "      \n",
        "    elif pos == 'ADJF':\n",
        "      try:\n",
        "        target_noun1 = sent_words[id_word+1]\n",
        "        p = morph.parse(target_noun1)[0]\n",
        "        pos1 = p.tag.POS \n",
        "      except IndexError:\n",
        "        pass\n",
        "      try:\n",
        "        target_noun2 = sent_words[id_word-2]\n",
        "        p = morph.parse(target_noun2)[0]\n",
        "        pos2 = p.tag.POS\n",
        "      except IndexError:\n",
        "        pass\n",
        "\n",
        "      if pos1 and pos1 == 'NOUN':\n",
        "        sim_service, sim_food = vect_sim(target_noun1, service_topic_vector, food_topic_vector)\n",
        "        if sim_service < 0.5 or sim_food < 0.5: #не берем совсем неблизкие к фуд и сервис штуки\n",
        "          if sim_service < sim_food:\n",
        "            asp_label = 'Service'\n",
        "          else:\n",
        "            asp_label = 'Food'\n",
        "        else:\n",
        "          asp_label = None\n",
        "        sim_pos, sim_neg = vect_sim(word, pos_vector, neg_vector)\n",
        "        if sim_pos < sim_neg:\n",
        "          label = 1\n",
        "        else: \n",
        "          label = 0\n",
        "      elif pos2 and pos2 == 'NOUN':\n",
        "        sim_service, sim_food = vect_sim(target_noun2, service_topic_vector, food_topic_vector)\n",
        "        if sim_service < 0.5 or sim_food < 0.5: #не берем совсем неблизкие к фуд и сервис штуки\n",
        "          if sim_service < sim_food:\n",
        "            asp_label = 'Service'\n",
        "          else:\n",
        "            asp_label = 'Food'\n",
        "        else:\n",
        "          asp_label = None\n",
        "        sim_pos, sim_neg = vect_sim(word, pos_vector, neg_vector)\n",
        "        if sim_pos < sim_neg:\n",
        "          label = 1\n",
        "        else: \n",
        "          label = 0\n",
        "      else:\n",
        "        label = None\n",
        "        asp_label = None\n",
        "      if abs(sim_pos - sim_neg) < 0.02:\n",
        "        label = None \n",
        "\n",
        "      current = str(id_sent + 1) + '\\t' + str(id_word + 1) + '\\t' + str(asp_label) + '\\t' + str(label)\n",
        "      result.add(current)\n",
        "      #print(current, word)\n",
        "    \n",
        "    if pos == 'NOUN':\n",
        "      #adj = None\n",
        "      pos1 = None\n",
        "      pos2 = None\n",
        "      try:\n",
        "        target_adj1 = sent_words[id_word-1]\n",
        "        p = morph.parse(target_adj1)[0]\n",
        "        pos1 = p.tag.POS\n",
        "        \n",
        "      except IndexError:\n",
        "        pass\n",
        "      try:\n",
        "        target_adj2 = sent_words[id_word+2]\n",
        "        p = morph.parse(target_adj2)[0]\n",
        "        pos2 = p.tag.POS\n",
        "      except IndexError:\n",
        "        pass\n",
        "      if pos1 and pos1 == 'ADJF':\n",
        "        adj = target_adj1\n",
        "        sim_pos, sim_neg = vect_sim(adj, pos_vector, neg_vector)\n",
        "        if sim_pos < sim_neg:\n",
        "          label = 1\n",
        "        else: \n",
        "          label = 0\n",
        "        if sent_words[sent_words.index(adj)-1] == 'не':\n",
        "          if label == 1:\n",
        "            label == 0\n",
        "          elif label == 0:\n",
        "            label = 1\n",
        "          id_word = str(sent_words.index(adj)) + ',' + str(sent_words.index(adj) + 1) #тк в юдпайпе не с нуля\n",
        "        else: \n",
        "          id_word = str(sent_words.index(adj) + 1)\n",
        "\n",
        "      elif pos2 and pos2 == 'ADJF':\n",
        "        adj = target_adj2\n",
        "        sim_pos, sim_neg = vect_sim(adj, pos_vector, neg_vector)\n",
        "        if sim_pos < sim_neg:\n",
        "          label = 1\n",
        "        else: \n",
        "          label = 0\n",
        "        if sent_words[sent_words.index(adj)-1] == 'не':\n",
        "          if label == 1:\n",
        "            label == 0\n",
        "          elif label == 0:\n",
        "            label = 1\n",
        "          id_word = str(sent_words.index(adj)) + ',' + str(sent_words.index(adj) + 1) #тк в юдпайпе не с нуля\n",
        "        else: \n",
        "          id_word = str(sent_words.index(adj) + 1)\n",
        "\n",
        "      else:\n",
        "        label = None\n",
        "      if abs(sim_pos - sim_neg) < 0.02:\n",
        "        label = None \n",
        "      current = str(id_sent + 1) + '\\t' + str(id_word) + '\\t' + str(asp_label) + '\\t' + str(label)\n",
        "      result.add(current)\n",
        "      #print(current, word)\n",
        "\n",
        "  final_result = set()\n",
        "  for i in result:\n",
        "    if \"None\" not in i:\n",
        "      final_result.add(i)\n",
        "\n",
        "  return final_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3UAMiRzEACw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# вот эту функцию можно применять на любом тексте и получать тональный словарь текста в переменной и файлом\n",
        "def final_classifier(file, result = set()):\n",
        "  middle_result = classifier1(file, result)\n",
        "  final_result = classifier2(file, middle_result)\n",
        "\n",
        "  new_file = file.split('.')[0] + '_done.txt'\n",
        "  with open(new_file, 'a', encoding = 'utf-8') as w:\n",
        "    for i in final_result:\n",
        "      w.write(i + '\\n')\n",
        "  return final_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHZufQphLQlc",
        "colab_type": "text"
      },
      "source": [
        "Ниже просто всякие проверки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3PW8U0esXgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_file = '/content/drive/My Drive/автобрея/6668.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkie31Ar6Dx0",
        "colab_type": "code",
        "outputId": "12ba6d87-d943-4a80-ccbf-d7be7c6ab82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "simple_result = classifier1(test_file)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cannot handle sentence 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96QD8HG5-1IF",
        "colab_type": "code",
        "outputId": "27d360a4-a118-45b8-fa16-29f426b21eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "simple_result"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1\\t4\\tFood\\t1',\n",
              " '1\\t4\\tService\\t1',\n",
              " '10\\t3\\tFood\\t1',\n",
              " '10\\t3\\tService\\t1',\n",
              " '11\\t3\\tFood\\t1',\n",
              " '11\\t3\\tService\\t1',\n",
              " '12\\t3\\tFood\\t1',\n",
              " '12\\t6\\tFood\\t1',\n",
              " '15\\t14\\tFood\\t1',\n",
              " '15\\t14\\tService\\t1',\n",
              " '15\\t3\\tFood\\t1',\n",
              " '15\\t4\\tService\\t1',\n",
              " '2\\t3\\tFood\\t0',\n",
              " '2\\t3\\tService\\t0',\n",
              " '2\\t31\\tFood\\t1',\n",
              " '2\\t31\\tService\\t1',\n",
              " '4\\t10\\tFood\\t1',\n",
              " '4\\t9\\tFood\\t1',\n",
              " '4\\t9\\tService\\t1',\n",
              " '5\\t12\\tService\\t1',\n",
              " '6\\t5\\tFood\\t1',\n",
              " '6\\t5\\tService\\t1',\n",
              " '6\\t7\\tService\\t1'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd5RGQ8XT1HT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "6dacdacb-f07f-454a-fc56-1c4a8eb9e76e"
      },
      "source": [
        "result_6668 = classifier2(test_file, simple_result)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR2DSvjTEc5U",
        "colab_type": "code",
        "outputId": "fee09009-97e8-4ef8-e8c9-6bf76bf2dee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(result_6668)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ2F2ct7EUHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxNL29aTFLpP",
        "colab_type": "code",
        "outputId": "18ed8ebd-4207-41fc-99b0-1d94b8e63b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "result_6668_2 = final_classifier(test_file)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cannot handle sentence 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWZocu_4FXys",
        "colab_type": "code",
        "outputId": "b66abd5d-64c2-4a5b-8f76-9822a31e71a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(result_6668_2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}